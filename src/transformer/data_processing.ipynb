{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from preprocessing.normalise import collect_id, normalize_id\n",
    "from utils.loader import load_raw_corpus\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "sys.setrecursionlimit(20000)\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_TYPE = set(\n",
    "    [\n",
    "        \"DebuggerStatement\",\n",
    "        \"ThisExpression\",\n",
    "        \"Super\",\n",
    "        \"EmptyStatement\",\n",
    "        \"Import\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 4579/14199 [00:10<01:15, 127.29it/s] /home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible set difference at position 2\n",
      "  ast = load_ast(code, file, ast_path)\n",
      " 40%|███▉      | 5622/14199 [00:11<00:08, 1046.24it/s]/home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible nested set at position 2\n",
      "  ast = load_ast(code, file, ast_path)\n",
      "/home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible nested set at position 1\n",
      "  ast = load_ast(code, file, ast_path)\n",
      "100%|██████████| 14199/14199 [00:28<00:00, 504.24it/s] \n",
      "100%|██████████| 14017/14017 [00:37<00:00, 375.63it/s] \n"
     ]
    }
   ],
   "source": [
    "corpus = load_raw_corpus(Path(\"../corpus/DIE\"))\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    id_idx = {\"v\": 0, \"f\": 0, \"c\": 0}\n",
    "    id_map = {}\n",
    "    collect_id(ast, id_map, id_idx)\n",
    "    normalize_id(ast, id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ASTBERTa/corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible set difference at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 1\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../ASTBERTa/corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14017/14017 [00:28<00:00, 495.07it/s] \n"
     ]
    }
   ],
   "source": [
    "from js_ast.fragmentise import node_to_frags\n",
    "\n",
    "\n",
    "frag_seqs: list[list[dict[str, Any]]] = []\n",
    "frag_info_seqs: list[list[tuple[int, str]]] = []\n",
    "all_node_types: set[str] = set()\n",
    "\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    frag_seq: list[dict[str, Any]] = []\n",
    "    node_types: set[str] = set()\n",
    "\n",
    "    node_to_frags(ast, frag_seq, node_types)\n",
    "\n",
    "    frag_seqs.append(frag_seq)\n",
    "    all_node_types.update(node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of frag_seqs: 14017\n",
      "Node types: {'LabeledStatement', 'LogicalExpression', 'Identifier', 'ArrayExpression', 'MemberExpression', 'BreakStatement', 'DoWhileStatement', 'AssignmentPattern', 'ForOfStatement', 'UpdateExpression', 'MethodDefinition', 'ClassDeclaration', 'YieldExpression', 'ForStatement', 'BinaryExpression', 'RestElement', 'ClassExpression', 'TryStatement', 'TemplateElement', 'Literal', 'ClassBody', 'WithStatement', 'MetaProperty', 'ConditionalExpression', 'ReturnStatement', 'ArrowFunctionExpression', 'FunctionExpression', 'NewExpression', 'AssignmentExpression', 'ForInStatement', 'VariableDeclaration', 'UnaryExpression', 'SwitchStatement', 'WhileStatement', 'ArrayPattern', 'TaggedTemplateExpression', 'TemplateLiteral', 'Program', 'AwaitExpression', 'ExpressionStatement', 'CatchClause', 'VariableDeclarator', 'FunctionDeclaration', 'SwitchCase', 'BlockStatement', 'ObjectExpression', 'ThrowStatement', 'ContinueStatement', 'ObjectPattern', 'Property', 'CallExpression', 'SequenceExpression', 'SpreadElement', 'IfStatement'}\n",
      "Max length of frag_seqs: [10029, 10029, 10029, 10029, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 11129, 11129, 11329, 11649, 11649, 13752, 13752, 14548, 14548, 14661, 14661, 15196, 15196, 17583, 17698, 18010, 18494, 18494, 19438, 20449, 20449, 21562, 21935, 22271, 28044, 31750, 32776, 32776, 40009, 47912, 47912, 47916, 47916, 49201, 54866, 55428, 55706, 55706, 59164, 65546, 98049, 114851, 116246, 117366, 118150, 120629, 120965, 121153, 122542, 122559, 196644, 200019, 213334, 213334]\n",
      "Min length of frag_seqs: [1, 1, 1, 1, 1]\n",
      "Avg length of frag_seqs: 577.7259042591139\n",
      "Percentage of frag_seqs below 1024: 0.8879218092316473\n"
     ]
    }
   ],
   "source": [
    "frag_seqs_len = list(sorted(map(lambda x: len(x), frag_seqs)))\n",
    "frag_seqs_below_max = [frag_seqs for x in frag_seqs if len(x) < 512]\n",
    "\n",
    "print(\"Length of frag_seqs:\", len(frag_seqs))\n",
    "print(\"Node types:\", all_node_types)\n",
    "\n",
    "print(\"Max length of frag_seqs:\", frag_seqs_len[-100:])\n",
    "print(\"Min length of frag_seqs:\", frag_seqs_len[:5])\n",
    "print(\"Avg length of frag_seqs:\", sum(frag_seqs_len) / len(frag_seqs_len))\n",
    "print(\"Percentage of frag_seqs below 1024:\", len(frag_seqs_below_max) / len(frag_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14017/14017 [00:23<00:00, 599.66it/s] \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "frag_freq: dict[str, int] = defaultdict(int)\n",
    "hash_to_frag: dict[str, dict[str, Any]] = {}\n",
    "frag_hash_to_type: dict[str, str] = {}\n",
    "\n",
    "for frag_seq in tqdm.tqdm(frag_seqs):\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        frag_freq[frag_hash] += 1\n",
    "\n",
    "        if frag_hash not in hash_to_frag:\n",
    "            hash_to_frag[frag_hash] = frag\n",
    "\n",
    "        if frag_hash not in frag_hash_to_type:\n",
    "            frag_hash_to_type[frag_hash] = frag[\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique fragments: 376817\n",
      "Max frequency: 446351\n",
      "Min frequency: 1\n",
      "Number of unique fragments with freq > 4: 16250\n",
      "[{'type': 'Literal', 'value': 'Done earlyReturnFromNestedTFTC', 'raw': '\"Done earlyReturnFromNestedTFTC\"', 'regex': None, 'bigint': None}, {'type': 'ArrayExpression', 'elements': [{'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}, {'type': 'FunctionExpression'}]}, {'type': 'Literal', 'value': 755, 'raw': '755', 'regex': None, 'bigint': None}, {'type': 'Identifier', 'name': 'console'}, {'type': 'BinaryExpression', 'operator': '<=', 'left': {'type': 'UnaryExpression'}, 'right': {'type': 'NewExpression'}}, {'type': 'Literal', 'value': 1198, 'raw': '1198', 'regex': None, 'bigint': None}, {'type': 'Identifier', 'name': 'read32'}, {'type': 'Literal', 'value': 'Symbol(Symbol.search)', 'raw': '\"Symbol(Symbol.search)\"', 'regex': None, 'bigint': None}, {'type': 'Identifier', 'name': 'v899'}, {'type': 'BinaryExpression', 'operator': '&', 'left': {'type': 'UnaryExpression'}, 'right': {'type': 'SequenceExpression'}}]\n"
     ]
    }
   ],
   "source": [
    "frag_freq_list = list(sorted(frag_freq.items(), reverse=True, key=lambda x: x[1]))\n",
    "oov_frags: list[str] = []\n",
    "\n",
    "# Add OOV anonymous frag type for those not in vocabulary\n",
    "for frag_type in all_node_types:\n",
    "    oov_frag = {\"type\": frag_type}\n",
    "    oov_frag_hash = hash_frag(oov_frag)\n",
    "    oov_frags.append(oov_frag_hash)\n",
    "    frag_hash_to_type[oov_frag_hash] = frag_type\n",
    "    hash_to_frag[oov_frag_hash] = oov_frag\n",
    "\n",
    "vocab_size = 20000\n",
    "\n",
    "threshold_frags = [frag_hash for frag_hash, freq in frag_freq_list if freq > 4]\n",
    "unique_vocab_frags = set(threshold_frags + oov_frags)\n",
    "\n",
    "vocab_frags = list(unique_vocab_frags)\n",
    "\n",
    "print(\"Number of unique fragments:\", len(frag_freq))\n",
    "print(\"Max frequency:\", max(frag_freq.values()))\n",
    "print(\"Min frequency:\", min(frag_freq.values()))\n",
    "\n",
    "print(\"Number of unique fragments with freq > 4:\", len(threshold_frags))\n",
    "print([hash_to_frag[frag] for frag in vocab_frags[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "CLS_TOKEN = \"<s>\"\n",
    "SEP_TOKEN = \"</s>\"\n",
    "MASK_TOKEN = \"<mask>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "special_tokens = [PAD_TOKEN, CLS_TOKEN, MASK_TOKEN, SEP_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ordered_vocab = special_tokens + list(vocab_frags)\n",
    "vocab = set(ordered_vocab)\n",
    "\n",
    "token_to_id = {token: i for i, token in enumerate(ordered_vocab)}\n",
    "id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "special_token_ids = set([token_to_id[token] for token in special_tokens])\n",
    "\n",
    "# Dictionary for fragment to type, special tokens are mapped to their string representation\n",
    "frag_id_to_type = {token_to_id[frag]: frag_hash_to_type[frag] for frag in vocab_frags}\n",
    "frag_id_to_frag = {token_to_id[frag]: hash_to_frag[frag] for frag in vocab_frags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_data = {\n",
    "    \"frag_seqs\": frag_seqs,\n",
    "    \"frag_id_to_type\": frag_id_to_type,\n",
    "    \"frag_id_to_frag\": frag_id_to_frag,\n",
    "}\n",
    "\n",
    "vocab_data = {\n",
    "    \"vocab\": vocab,\n",
    "    \"token_to_id\": token_to_id,\n",
    "    \"id_to_token\": id_to_token,\n",
    "    \"special_token_ids\": special_token_ids,\n",
    "}\n",
    "\n",
    "pickle.dump(frag_data, open(\"../ASTBERTa/frag_data_old.pkl\", \"wb\"))\n",
    "pickle.dump(vocab_data, open(\"../ASTBERTa/vocab_data_old.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ASTBERTa/frag_data_old.pkl\", \"rb\") as f:\n",
    "    frag_data = pickle.load(f)\n",
    "\n",
    "frag_seqs = frag_data[\"frag_seqs\"]\n",
    "frag_id_to_type = frag_data[\"frag_id_to_type\"]\n",
    "frag_id_to_frag = frag_data[\"frag_id_to_frag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29289\n"
     ]
    }
   ],
   "source": [
    "print(len(frag_id_to_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29294\n",
      "[('11d846149aa81fe2e9510fd04e20c4d2ab934ee11cebd55a08652f2ca340a696', 5), ('9835be32ffc8f319ad5b0145a24af482a4c50ebb13b4ad7c74a49675a93d6d9f', 6), ('b16b7634a2989fce41070893f537b174da89a512cc3cae2dbd065ecf7f683d3e', 7), ('d99c8e32260126c9b9409e3165d75da6f258650a7d02dadb4ab7f7e3debf120c', 8), ('b5a305c3c5791c44f3dd7f9bd11c3ebc0181005319f4ab099fae5f8b2e272a8a', 9), ('da680ac8460e95f0a5b148f82dd93bcfa2bba11ca23351e809597bc78f278a7f', 10), ('75e01a796900fc042894b6835874e7d5a4c8614532d4240974a2095a2dbf9ba4', 11), ('4b1e068e3f69f5d94f1851f66b69414ab4a82bbc40084696177833f1ccb7b7d4', 12), ('6a1a0fb3138dafcf29a526a7a179b760aa3b3c351e8089c549408c52beebd10f', 13), ('8628b19a279dbf1edd748bf47b919b8f9eca4817db1fd654557ab55683e367f4', 14)]\n"
     ]
    }
   ],
   "source": [
    "vocab = set(list(special_token_ids) + list(frag_id_to_type.keys()))\n",
    "print(len(vocab))\n",
    "\n",
    "token_to_id = {}\n",
    "\n",
    "for frag_id, frag in frag_id_to_frag.items():\n",
    "    token_to_id[hash_frag(frag)] = frag_id\n",
    "\n",
    "for i, item in enumerate(special_tokens):\n",
    "    token_to_id[item] = i\n",
    "\n",
    "id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "special_token_ids = set([token_to_id[token] for token in special_tokens])\n",
    "print(list(token_to_id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible set difference at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 1\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "with open(\"../ASTBERTa/frag_data.pkl\", \"rb\") as f:\n",
    "    frag_data = pickle.load(f)\n",
    "\n",
    "with open(\"../ASTBERTa/vocab_data.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "frag_seqs = frag_data[\"frag_seqs\"]\n",
    "frag_id_to_type = frag_data[\"frag_id_to_type\"]\n",
    "frag_id_to_frag = frag_data[\"frag_id_to_frag\"]\n",
    "\n",
    "vocab = vocab_data[\"vocab\"]\n",
    "token_to_id = vocab_data[\"token_to_id\"]\n",
    "id_to_token = vocab_data[\"id_to_token\"]\n",
    "special_token_ids = vocab_data[\"special_token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14017/14017 [00:23<00:00, 601.91it/s] \n"
     ]
    }
   ],
   "source": [
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "data: list[list[int]] = []\n",
    "\n",
    "for frag_seq in tqdm.tqdm(frag_seqs):\n",
    "    seq: list[int] = []\n",
    "\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        if frag_hash in vocab:\n",
    "            seq.append(token_to_id[frag_hash])\n",
    "        else:\n",
    "            oov_frag = {\"type\": frag_hash_to_type[frag_hash]}\n",
    "            oov_frag_hash = hash_frag(oov_frag)\n",
    "            if oov_frag_hash in vocab:\n",
    "                seq.append(token_to_id[oov_frag_hash])\n",
    "            else:\n",
    "                print(\"UNK_TOKEN\")\n",
    "                seq.append(token_to_id[UNK_TOKEN])\n",
    "\n",
    "    data.append([token_to_id[CLS_TOKEN]] + seq + [token_to_id[SEP_TOKEN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(data, open(\"../ASTBERTa/data_old.pkl\", \"wb\"))\n",
    "# pickle.dump(token_to_id, open(\"ASTBERTa/token_to_id.pkl\", \"wb\"))\n",
    "# pickle.dump(vocab, open(\"ASTBERTa/vocab.pkl\", \"wb\"))\n",
    "# pickle.dump(hash_to_frag, open(\"ASTBERTa/hash_to_frag.pkl\", \"wb\"))\n",
    "# pickle.dump(id_to_token, open(\"ASTBERTa/id_to_token.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "js-rl-vfj9GiAe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
