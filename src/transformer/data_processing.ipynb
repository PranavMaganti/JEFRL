{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from preprocessing.normalise import collect_id, normalize_id\n",
    "from utils.loader import load_raw_corpus\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "sys.setrecursionlimit(20000)\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_TYPE = set(\n",
    "    [\n",
    "        \"DebuggerStatement\",\n",
    "        \"ThisExpression\",\n",
    "        \"Super\",\n",
    "        \"EmptyStatement\",\n",
    "        \"Import\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 4584/14199 [00:24<02:52, 55.66it/s] /home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible set difference at position 2\n",
      "  ast = load_ast(code, file, ast_path)\n",
      " 40%|████      | 5721/14199 [00:31<00:28, 302.62it/s]/home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible nested set at position 2\n",
      "  ast = load_ast(code, file, ast_path)\n",
      "/home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible nested set at position 1\n",
      "  ast = load_ast(code, file, ast_path)\n",
      "100%|██████████| 14199/14199 [01:10<00:00, 201.53it/s]\n",
      "100%|██████████| 14017/14017 [00:37<00:00, 372.40it/s] \n"
     ]
    }
   ],
   "source": [
    "corpus = load_raw_corpus(Path(\"../corpus/DIE\"))\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    id_idx = {\"v\": 0, \"f\": 0, \"c\": 0}\n",
    "    id_map = {}\n",
    "    collect_id(ast, id_map, id_idx)\n",
    "    normalize_id(ast, id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ASTBERTa/corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.10/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible set difference at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.10/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.10/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 1\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../ASTBERTa/corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14017/14017 [00:27<00:00, 501.57it/s] \n"
     ]
    }
   ],
   "source": [
    "from js_ast.fragmentise import node_to_frags\n",
    "\n",
    "\n",
    "frag_seqs: list[list[dict[str, Any]]] = []\n",
    "frag_info_seqs: list[list[tuple[int, str]]] = []\n",
    "all_node_types: set[str] = set()\n",
    "\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    frag_seq: list[dict[str, Any]] = []\n",
    "    node_types: set[str] = set()\n",
    "\n",
    "    node_to_frags(ast, frag_seq, node_types)\n",
    "\n",
    "    frag_seqs.append(frag_seq)\n",
    "    all_node_types.update(node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of frag_seqs: 14017\n",
      "Node types: {'ClassBody', 'AssignmentPattern', 'YieldExpression', 'ThrowStatement', 'ClassExpression', 'ObjectPattern', 'MemberExpression', 'WhileStatement', 'AwaitExpression', 'Literal', 'TryStatement', 'MetaProperty', 'ArrayPattern', 'BlockStatement', 'Property', 'CallExpression', 'ForStatement', 'Program', 'SequenceExpression', 'NewExpression', 'DoWhileStatement', 'ArrayExpression', 'LogicalExpression', 'ObjectExpression', 'FunctionExpression', 'TemplateLiteral', 'ClassDeclaration', 'MethodDefinition', 'SwitchStatement', 'ContinueStatement', 'ReturnStatement', 'SwitchCase', 'BreakStatement', 'ForOfStatement', 'ConditionalExpression', 'TaggedTemplateExpression', 'VariableDeclarator', 'IfStatement', 'ArrowFunctionExpression', 'ExpressionStatement', 'VariableDeclaration', 'TemplateElement', 'BinaryExpression', 'RestElement', 'UnaryExpression', 'AssignmentExpression', 'LabeledStatement', 'UpdateExpression', 'CatchClause', 'Identifier', 'WithStatement', 'SpreadElement', 'FunctionDeclaration', 'ForInStatement'}\n",
      "Max length of frag_seqs: [10029, 10029, 10029, 10029, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 11129, 11129, 11329, 11649, 11649, 13752, 13752, 14548, 14548, 14661, 14661, 15196, 15196, 17583, 17698, 18010, 18494, 18494, 19438, 20449, 20449, 21562, 21935, 22271, 28044, 31750, 32776, 32776, 40009, 47912, 47912, 47916, 47916, 49201, 54866, 55428, 55706, 55706, 59164, 65546, 98049, 114851, 116246, 117366, 118150, 120629, 120965, 121153, 122542, 122559, 196644, 200019, 213334, 213334]\n",
      "Min length of frag_seqs: [1, 1, 1, 1, 1]\n",
      "Avg length of frag_seqs: 577.7259042591139\n",
      "Percentage of frag_seqs below 1024: 0.8879218092316473\n"
     ]
    }
   ],
   "source": [
    "frag_seqs_len = list(sorted(map(lambda x: len(x), frag_seqs)))\n",
    "frag_seqs_below_max = [frag_seqs for x in frag_seqs if len(x) < 512]\n",
    "\n",
    "print(\"Length of frag_seqs:\", len(frag_seqs))\n",
    "print(\"Node types:\", all_node_types)\n",
    "\n",
    "print(\"Max length of frag_seqs:\", frag_seqs_len[-100:])\n",
    "print(\"Min length of frag_seqs:\", frag_seqs_len[:5])\n",
    "print(\"Avg length of frag_seqs:\", sum(frag_seqs_len) / len(frag_seqs_len))\n",
    "print(\"Percentage of frag_seqs below 1024:\", len(frag_seqs_below_max) / len(frag_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14017/14017 [00:24<00:00, 575.14it/s] \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "frag_freq: dict[str, int] = defaultdict(int)\n",
    "hash_to_frag: dict[str, dict[str, Any]] = {}\n",
    "frag_hash_to_type: dict[str, str] = {}\n",
    "\n",
    "for frag_seq in tqdm.tqdm(frag_seqs):\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        frag_freq[frag_hash] += 1\n",
    "\n",
    "        if frag_hash not in hash_to_frag:\n",
    "            hash_to_frag[frag_hash] = frag\n",
    "\n",
    "        if frag_hash not in frag_hash_to_type:\n",
    "            frag_hash_to_type[frag_hash] = frag[\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique fragments: 376817\n",
      "Max frequency: 446351\n",
      "Min frequency: 1\n",
      "Number of unique fragments with freq > 5: 29289\n",
      "[{'type': 'Literal', 'value': 'ok', 'raw': \"'ok'\", 'regex': None, 'bigint': None}, {'type': 'Literal', 'value': '__hgzm=144631658.1231367708.1.1.hgzpfe=(qverpg)|hgzppa=(qverpg)|hgzpzq=(abar)', 'raw': \"'__hgzm=144631658.1231367708.1.1.hgzpfe=(qverpg)|hgzppa=(qverpg)|hgzpzq=(abar)'\", 'regex': None, 'bigint': None}, {'type': 'Literal', 'value': '\\udfff', 'raw': '\"\\\\udfff\"', 'regex': None, 'bigint': None}, {'type': 'Literal', 'value': 'getOwnPropertyDescriptor', 'raw': '\"getOwnPropertyDescriptor\"', 'regex': None, 'bigint': None}, {'type': 'Literal', 'value': re.compile('Ԩ', re.IGNORECASE), 'raw': '/\\\\u0528/i', 'regex': {'pattern': '\\\\u0528', 'flags': 'i'}, 'bigint': None}, {'type': 'Literal', 'value': 914, 'raw': '914', 'regex': None, 'bigint': None}, {'type': 'LabeledStatement', 'label': {'type': 'Identifier'}, 'body': {'type': 'DoWhileStatement'}}, {'type': 'Identifier', 'name': 'f1043'}, {'type': 'Literal', 'value': 'Ӥ', 'raw': \"'\\\\u04e4'\", 'regex': None, 'bigint': None}, {'type': 'Identifier', 'name': 'f983'}]\n"
     ]
    }
   ],
   "source": [
    "frag_freq_list = list(sorted(frag_freq.items(), reverse=True, key=lambda x: x[1]))\n",
    "unique_vocab_frags = set([frag_hash for frag_hash, freq in frag_freq_list if freq > 3])\n",
    "oov_frags: list[str] = []\n",
    "\n",
    "# Add OOV anonymous frag type for those not in vocabulary\n",
    "for frag_type in all_node_types:\n",
    "    oov_frag = {\"type\": frag_type}\n",
    "    oov_frag_hash = hash_frag(oov_frag)\n",
    "    oov_frags.append(oov_frag_hash)\n",
    "    frag_hash_to_type[oov_frag_hash] = frag_type\n",
    "    hash_to_frag[oov_frag_hash] = oov_frag\n",
    "\n",
    "unique_vocab_frags.update(oov_frags)\n",
    "vocab_frags = list(unique_vocab_frags)\n",
    "\n",
    "print(\"Number of unique fragments:\", len(frag_freq))\n",
    "print(\"Max frequency:\", max(frag_freq.values()))\n",
    "print(\"Min frequency:\", min(frag_freq.values()))\n",
    "\n",
    "print(\"Number of unique fragments with freq > 5:\", len(vocab_frags))\n",
    "print([hash_to_frag[frag] for frag in vocab_frags[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "CLS_TOKEN = \"<s>\"\n",
    "SEP_TOKEN = \"</s>\"\n",
    "MASK_TOKEN = \"<mask>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "special_tokens = [PAD_TOKEN, CLS_TOKEN, MASK_TOKEN, SEP_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ordered_vocab = special_tokens + list(vocab_frags)\n",
    "vocab = set(ordered_vocab)\n",
    "\n",
    "token_to_id = {token: i for i, token in enumerate(ordered_vocab)}\n",
    "id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "special_token_ids = set([token_to_id[token] for token in special_tokens])\n",
    "\n",
    "# Dictionary for fragment to type, special tokens are mapped to their string representation\n",
    "frag_id_to_type = {token_to_id[frag]: frag_hash_to_type[frag] for frag in vocab_frags}\n",
    "frag_id_to_frag = {token_to_id[frag]: hash_to_frag[frag] for frag in vocab_frags}\n",
    "frag_type_to_id = {}\n",
    "\n",
    "for frag_id, frag_type in frag_id_to_type.items():\n",
    "    if frag_type not in frag_type_to_id:\n",
    "        frag_type_to_id[frag_type] = set()\n",
    "\n",
    "    frag_type_to_id[frag_type].add(frag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_data = {\n",
    "    \"frag_seqs\": frag_seqs,\n",
    "    \"frag_id_to_type\": frag_id_to_type,\n",
    "    \"frag_id_to_frag\": frag_id_to_frag,\n",
    "    \"frag_type_to_id\": frag_type_to_id,\n",
    "}\n",
    "\n",
    "vocab_data = {\n",
    "    \"vocab\": vocab,\n",
    "    \"token_to_id\": token_to_id,\n",
    "    \"id_to_token\": id_to_token,\n",
    "    \"special_token_ids\": special_token_ids,\n",
    "}\n",
    "\n",
    "pickle.dump(frag_data, open(\"../ASTBERTa/frag_data.pkl\", \"wb\"))\n",
    "pickle.dump(vocab_data, open(\"../ASTBERTa/vocab_data.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "with open(\"../ASTBERTa/frag_data.pkl\", \"rb\") as f:\n",
    "    frag_data = pickle.load(f)\n",
    "\n",
    "with open(\"../ASTBERTa/vocab_data.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "\n",
    "frag_seqs = frag_data[\"frag_seqs\"]\n",
    "frag_id_to_type = frag_data[\"frag_id_to_type\"]\n",
    "frag_id_to_frag = frag_data[\"frag_id_to_frag\"]\n",
    "frag_type_to_id = frag_data[\"frag_type_to_id\"]\n",
    "\n",
    "vocab = vocab_data[\"vocab\"]\n",
    "token_to_id = vocab_data[\"token_to_id\"]\n",
    "id_to_token = vocab_data[\"id_to_token\"]\n",
    "special_token_ids = vocab_data[\"special_token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14017 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'frag_hash_to_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     seq\u001b[39m.\u001b[39mappend(token_to_id[frag_hash])\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     oov_frag \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: frag_hash_to_type[frag_hash]}\n\u001b[1;32m     14\u001b[0m     oov_frag_hash \u001b[39m=\u001b[39m hash_frag(oov_frag)\n\u001b[1;32m     15\u001b[0m     \u001b[39mif\u001b[39;00m oov_frag_hash \u001b[39min\u001b[39;00m vocab:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frag_hash_to_type' is not defined"
     ]
    }
   ],
   "source": [
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "data: list[list[int]] = []\n",
    "\n",
    "for frag_seq in tqdm.tqdm(frag_seqs):\n",
    "    seq: list[int] = []\n",
    "\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        if frag_hash in vocab:\n",
    "            seq.append(token_to_id[frag_hash])\n",
    "        else:\n",
    "            oov_frag = {\"type\": frag_hash_to_type[frag_hash]}\n",
    "            oov_frag_hash = hash_frag(oov_frag)\n",
    "            if oov_frag_hash in vocab:\n",
    "                seq.append(token_to_id[oov_frag_hash])\n",
    "            else:\n",
    "                print(\"UNK_TOKEN\")\n",
    "                seq.append(token_to_id[UNK_TOKEN])\n",
    "\n",
    "    data.append([token_to_id[CLS_TOKEN]] + seq + [token_to_id[SEP_TOKEN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(data, open(\"../ASTBERTa/data.pkl\", \"wb\"))\n",
    "# pickle.dump(token_to_id, open(\"ASTBERTa/token_to_id.pkl\", \"wb\"))\n",
    "# pickle.dump(vocab, open(\"ASTBERTa/vocab.pkl\", \"wb\"))\n",
    "# pickle.dump(hash_to_frag, open(\"ASTBERTa/hash_to_frag.pkl\", \"wb\"))\n",
    "# pickle.dump(id_to_token, open(\"ASTBERTa/id_to_token.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "js-rl-vfj9GiAe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
