{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from preprocessing.normalise import collect_id, normalize_id\n",
    "from utils.loader import load_raw_corpus\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "sys.setrecursionlimit(20000)\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_TYPE = [\n",
    "    \"DebuggerStatement\",\n",
    "    \"ThisExpression\",\n",
    "    \"Super\",\n",
    "    \"EmptyStatement\",\n",
    "    \"Import\",\n",
    "]\n",
    "\n",
    "MAX_SEQ_LEN = 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 5784/18162 [00:11<00:26, 472.40it/s] /home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible set difference at position 2\n",
      "  ast = load_ast(code, ast_path)\n",
      " 94%|█████████▎| 17003/18162 [01:01<00:03, 291.18it/s] /home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible nested set at position 2\n",
      "  ast = load_ast(code, ast_path)\n",
      "/home/pranav/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/utils/loader.py:30: FutureWarning: Possible nested set at position 1\n",
      "  ast = load_ast(code, ast_path)\n",
      "100%|██████████| 18162/18162 [01:02<00:00, 289.33it/s] \n",
      " 61%|██████▏   | 11016/17942 [00:21<00:13, 502.81it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m   id_idx \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mv\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m\n\u001b[1;32m      7\u001b[0m   }\n\u001b[1;32m      8\u001b[0m   id_map \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> 9\u001b[0m   collect_id(ast, id_map, id_idx)\n\u001b[1;32m     10\u001b[0m   normalize_id(ast, id_map)\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mASTBERTa/corpus.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/preprocessing/normalise.py:46\u001b[0m, in \u001b[0;36mcollect_id\u001b[0;34m(node, id_dict, id_cnt, prop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m val:\n\u001b[1;32m     45\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(child, Node):\n\u001b[0;32m---> 46\u001b[0m                 collect_id(child, id_dict, id_cnt, field)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39mparent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m is_declared_id(node, prop):\n\u001b[1;32m     49\u001b[0m     \u001b[39mmatch\u001b[39;00m node\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mtype:\n",
      "File \u001b[0;32m~/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/preprocessing/normalise.py:46\u001b[0m, in \u001b[0;36mcollect_id\u001b[0;34m(node, id_dict, id_cnt, prop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m val:\n\u001b[1;32m     45\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(child, Node):\n\u001b[0;32m---> 46\u001b[0m                 collect_id(child, id_dict, id_cnt, field)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39mparent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m is_declared_id(node, prop):\n\u001b[1;32m     49\u001b[0m     \u001b[39mmatch\u001b[39;00m node\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mtype:\n",
      "File \u001b[0;32m~/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/preprocessing/normalise.py:46\u001b[0m, in \u001b[0;36mcollect_id\u001b[0;34m(node, id_dict, id_cnt, prop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m val:\n\u001b[1;32m     45\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(child, Node):\n\u001b[0;32m---> 46\u001b[0m                 collect_id(child, id_dict, id_cnt, field)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39mparent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m is_declared_id(node, prop):\n\u001b[1;32m     49\u001b[0m     \u001b[39mmatch\u001b[39;00m node\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mtype:\n",
      "File \u001b[0;32m~/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/preprocessing/normalise.py:39\u001b[0m, in \u001b[0;36mcollect_id\u001b[0;34m(node, id_dict, id_cnt, prop)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect_id\u001b[39m(\n\u001b[1;32m     32\u001b[0m     node: Node,\n\u001b[1;32m     33\u001b[0m     id_dict: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m ):\n\u001b[1;32m     37\u001b[0m     \u001b[39m# Tree traversal\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m node\u001b[39m.\u001b[39mfields:\n\u001b[0;32m---> 39\u001b[0m         val \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(node, field)\n\u001b[1;32m     41\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(val, Node):\n\u001b[1;32m     42\u001b[0m             collect_id(val, id_dict, id_cnt, field)\n",
      "File \u001b[0;32m~/Projects/Uni/Year_4/Computing/FinarYearProject/js-rl/src/js_ast/nodes.py:55\u001b[0m, in \u001b[0;36mNode.type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(node, Node):\n\u001b[1;32m     53\u001b[0m                     node\u001b[39m.\u001b[39mparent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m---> 55\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     57\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"The name of the node type, e.g. 'Identifier'.\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = load_raw_corpus(Path(\"data/DIE-corpus/\"))\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    id_idx = {\"v\": 0, \"f\": 0, \"c\": 0}\n",
    "    id_map = {}\n",
    "    collect_id(ast, id_map, id_idx)\n",
    "    normalize_id(ast, id_map)\n",
    "\n",
    "with open(\"ASTBERTa/corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible set difference at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 1\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "with open(\"ASTBERTa/corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17942/17942 [00:27<00:00, 653.33it/s] \n"
     ]
    }
   ],
   "source": [
    "from js_ast.nodes import Node\n",
    "\n",
    "\n",
    "subtrees: list[Node] = []\n",
    "\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    for node in ast.traverse():\n",
    "        subtrees.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17942/17942 [00:56<00:00, 319.88it/s] \n"
     ]
    }
   ],
   "source": [
    "from js_ast.fragmentise import node_to_frags\n",
    "\n",
    "\n",
    "frag_seqs: list[list[dict[str, Any]]] = []\n",
    "frag_info_seqs: list[list[tuple[int, str]]] = []\n",
    "all_node_types: set[str] = set()\n",
    "\n",
    "\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    frag_seq: list[dict[str, Any]] = []\n",
    "    frag_info_seq: list[tuple[int, str]] = []\n",
    "    node_types: set[str] = set()\n",
    "\n",
    "    node_to_frags(ast, frag_seq, frag_info_seq, node_types)\n",
    "\n",
    "    frag_seqs.append(frag_seq)\n",
    "    frag_info_seqs.append(frag_info_seq)\n",
    "    all_node_types.update(node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of frag_seqs: 17942\n",
      "Length of frag_info_seqs: 17942\n",
      "Node types: {'ObjectPattern', 'ArrayPattern', 'VariableDeclarator', 'NewExpression', 'ClassDeclaration', 'TemplateElement', 'MetaProperty', 'TryStatement', 'Literal', 'FunctionExpression', 'LogicalExpression', 'ForInStatement', 'WithStatement', 'CallExpression', 'ConditionalExpression', 'AssignmentPattern', 'ArrayExpression', 'MethodDefinition', 'RestElement', 'ContinueStatement', 'ClassBody', 'UnaryExpression', 'UpdateExpression', 'ObjectExpression', 'AssignmentExpression', 'ForOfStatement', 'ThrowStatement', 'ForStatement', 'TemplateLiteral', 'BlockStatement', 'CatchClause', 'Program', 'Identifier', 'ClassExpression', 'ReturnStatement', 'BinaryExpression', 'DoWhileStatement', 'BreakStatement', 'TaggedTemplateExpression', 'LabeledStatement', 'ArrowFunctionExpression', 'SequenceExpression', 'VariableDeclaration', 'IfStatement', 'WhileStatement', 'Property', 'SwitchStatement', 'SwitchCase', 'FunctionDeclaration', 'ExpressionStatement', 'MemberExpression', 'AwaitExpression', 'YieldExpression', 'SpreadElement'}\n",
      "Max length of frag_seqs: [10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 10474, 11129, 11129, 11329, 11649, 11649, 11985, 13752, 13752, 14548, 14548, 14661, 14661, 15196, 15196, 15754, 17583, 17698, 18010, 18115, 18121, 18494, 18494, 19438, 19848, 20449, 20449, 21562, 21562, 21925, 21935, 22271, 22462, 28044, 31750, 32776, 32776, 32802, 40009, 45338, 47912, 47912, 47916, 47916, 49201, 54866, 55428, 55428, 55706, 55706, 59164, 65546, 98049, 114851, 116246, 117366, 118150, 120629, 120965, 121153, 122542, 122542, 122559, 122559, 196644, 196673, 200019, 213334, 213334, 488372]\n",
      "Min length of frag_seqs: [1, 1, 1, 1, 1]\n",
      "Avg length of frag_seqs: 553.9651097982388\n",
      "Percentage of frag_seqs below 1024: 0.901460260840486\n"
     ]
    }
   ],
   "source": [
    "frag_seqs_len = list(sorted(map(lambda x: len(x), frag_seqs)))\n",
    "frag_seqs_below_max = [frag_seqs for x in frag_seqs if len(x) < 512]\n",
    "\n",
    "print(\"Length of frag_seqs:\", len(frag_seqs))\n",
    "print(\"Length of frag_info_seqs:\", len(frag_info_seqs))\n",
    "print(\"Node types:\", all_node_types)\n",
    "\n",
    "print(\"Max length of frag_seqs:\", frag_seqs_len[-100:])\n",
    "print(\"Min length of frag_seqs:\", frag_seqs_len[:5])\n",
    "print(\"Avg length of frag_seqs:\", sum(frag_seqs_len) / len(frag_seqs_len))\n",
    "print(\"Percentage of frag_seqs below 1024:\", len(frag_seqs_below_max) / len(frag_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17942/17942 [00:30<00:00, 597.58it/s] \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "frag_freq: dict[str, int] = defaultdict(int)\n",
    "hash_to_frag: dict[str, dict[str, Any]] = {}\n",
    "frag_hash_to_type: dict[str, str] = {}\n",
    "\n",
    "for frag_seq, frag_info_seq in tqdm.tqdm(\n",
    "    zip(frag_seqs, frag_info_seqs), total=len(frag_seqs)\n",
    "):\n",
    "    assert len(frag_seq) == len(frag_info_seq)\n",
    "    for frag, (_, frag_type) in zip(frag_seq, frag_info_seq):\n",
    "        frag_hash = hash_frag(frag)\n",
    "        frag_freq[frag_hash] += 1\n",
    "\n",
    "        if frag_hash not in hash_to_frag:\n",
    "            hash_to_frag[frag_hash] = frag\n",
    "\n",
    "        if frag_hash not in frag_hash_to_type:\n",
    "            frag_hash_to_type[frag_hash] = frag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique fragments: 481301\n",
      "Max frequency: 512920\n",
      "Min frequency: 1\n",
      "Number of unique fragments with freq > 5: 20537\n",
      "[{'type': 'Literal', 'value': 1801, 'raw': '1801', 'regex': None, 'bigint': None}, {'type': 'Identifier', 'name': 'uniformContext'}, {'type': 'SwitchStatement', 'discriminant': {'type': 'Identifier'}, 'cases': [{'type': 'SwitchCase'}, {'type': 'SwitchCase'}, {'type': 'SwitchCase'}, {'type': 'SwitchCase'}, {'type': 'SwitchCase'}]}, {'type': 'Identifier', 'name': 'v810'}, {'type': 'Identifier', 'name': 'f2167'}, {'type': 'Literal', 'value': 3.3, 'raw': '3.3', 'regex': None, 'bigint': None}, {'type': 'Identifier', 'name': 'blah'}, {'type': 'Literal', 'value': 'search', 'raw': '\"search\"', 'regex': None, 'bigint': None}, {'type': 'BinaryExpression', 'operator': '<=', 'left': {'type': 'MemberExpression'}, 'right': {'type': 'MemberExpression'}}, {'type': 'BinaryExpression', 'operator': '<', 'left': {'type': 'CallExpression'}, 'right': {'type': 'Literal'}}]\n"
     ]
    }
   ],
   "source": [
    "frag_freq_list = list(sorted(frag_freq.items(), reverse=True, key=lambda x: x[1]))\n",
    "unique_vocab_frags = set([frag_hash for frag_hash, freq in frag_freq_list if freq > 5])\n",
    "oov_frags: list[str] = []\n",
    "\n",
    "# Add OOV anonymous frag type for those not in vocabulary\n",
    "for frag_type in all_node_types:\n",
    "    oov_frag = {\"type\": frag_type}\n",
    "    oov_frag_hash = hash_frag(oov_frag)\n",
    "    oov_frags.append(oov_frag_hash)\n",
    "    frag_hash_to_type[oov_frag_hash] = frag_type\n",
    "\n",
    "unique_vocab_frags.update(oov_frags)\n",
    "\n",
    "vocab_frags = list(unique_vocab_frags)\n",
    "\n",
    "print(\"Number of unique fragments:\", len(frag_freq))\n",
    "print(\"Max frequency:\", max(frag_freq.values()))\n",
    "print(\"Min frequency:\", min(frag_freq.values()))\n",
    "\n",
    "print(\"Number of unique fragments with freq > 5:\", len(vocab_frags))\n",
    "print([hash_to_frag[frag] for frag in vocab_frags[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "CLS_TOKEN = \"<s>\"\n",
    "SEP_TOKEN = \"</s>\"\n",
    "MASK_TOKEN = \"<mask>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "special_tokens = [PAD_TOKEN, CLS_TOKEN, MASK_TOKEN, SEP_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ordered_vocab = special_tokens + list(vocab_frags)\n",
    "vocab = set(ordered_vocab)\n",
    "\n",
    "token_to_id = {token: i for i, token in enumerate(ordered_vocab)}\n",
    "id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "special_token_ids = set([token_to_id[token] for token in special_tokens])\n",
    "\n",
    "# Dictionary for fragment to type, special tokens are mapped to their string representation\n",
    "frag_type_to_ids: dict[str, list[int]] = defaultdict(list)\n",
    "\n",
    "for i, frag_hash in enumerate(vocab_frags):\n",
    "    frag_type = frag_hash_to_type[frag_hash]\n",
    "    frag_type_to_ids[frag_type].append(token_to_id[frag_hash])\n",
    "    frag_id_to_type = {i: frag_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_data = {\n",
    "    \"frag_seqs\": frag_seqs,\n",
    "    \"frag_info_seqs\": frag_info_seqs,\n",
    "}\n",
    "\n",
    "vocab_data = {\n",
    "    \"vocab\": vocab,\n",
    "    \"token_to_id\": token_to_id,\n",
    "    \"id_to_token\": id_to_token,\n",
    "    \"special_token_ids\": special_token_ids,\n",
    "    \"frag_type_to_ids\": frag_type_to_ids,\n",
    "    \"frag_id_to_type\": frag_id_to_type,\n",
    "}\n",
    "\n",
    "pickle.dump(frag_data, open(\"ASTBERTa/frag_data.pkl\", \"wb\"))\n",
    "pickle.dump(vocab_data, open(\"ASTBERTa/vocab_data.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible set difference at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 1\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "with open(\"ASTBERTa/frag_data.pkl\", \"rb\") as f:\n",
    "    frag_data = pickle.load(f)\n",
    "\n",
    "with open(\"ASTBERTa/vocab_data.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "\n",
    "frag_seqs = frag_data[\"frag_seqs\"]\n",
    "frag_info_seqs = frag_data[\"frag_info_seqs\"]\n",
    "\n",
    "token_to_id = vocab_data[\"token_to_id\"]\n",
    "id_to_token = vocab_data[\"id_to_token\"]\n",
    "special_token_ids = vocab_data[\"special_token_ids\"]\n",
    "vocab = vocab_data[\"vocab\"]\n",
    "frag_type_to_ids = vocab_data[\"frag_type_to_ids\"]\n",
    "frag_id_to_type = vocab_data[\"frag_id_to_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4658283/9728634 [04:13<18:23, 4596.25it/s]  "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from js_ast.fragmentise import node_to_frags\n",
    "\n",
    "\n",
    "subtree_frag_seqs: list[list[dict[str, Any]]] = []\n",
    "subtree_frag_info_seqs: list[list[tuple[int, str]]] = []\n",
    "\n",
    "for ast in tqdm.tqdm(subtrees):\n",
    "    subtree_frag_seq: list[dict[str, Any]] = []\n",
    "    subtree_frag_info_seq: list[tuple[int, str]] = []\n",
    "\n",
    "    node_to_frags(ast, subtree_frag_seq, subtree_frag_info_seq, set())\n",
    "\n",
    "    subtree_frag_seqs.append(subtree_frag_seq[:MAX_SEQ_LEN])\n",
    "    subtree_frag_info_seqs.append(subtree_frag_info_seq[:MAX_SEQ_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of subtree_frag_seqs: 59\n",
      "Min length of subtree_frag_seqs: 59\n",
      "Avg length of subtree_frag_seqs: 59.0\n",
      "Number of seqs with length < 5: 0\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(frag_seq) for frag_seq in subtree_frag_seqs]\n",
    "\n",
    "print(\"Max length of subtree_frag_seqs:\", max(lengths))\n",
    "print(\"Min length of subtree_frag_seqs:\", min(lengths))\n",
    "print(\"Avg length of subtree_frag_seqs:\", sum(lengths) / len(lengths))\n",
    "print(\"Number of seqs with length = 1:\", len([l for l in lengths if l < 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9728634/9728634 [24:44<00:00, 6555.56it/s]  \n"
     ]
    }
   ],
   "source": [
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "data: list[list[int]] = []\n",
    "\n",
    "for frag_seq in tqdm.tqdm(subtree_frag_seqs):\n",
    "    seq: list[int] = []\n",
    "\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        if frag_hash in vocab:\n",
    "            seq.append(token_to_id[frag_hash])\n",
    "        else:\n",
    "            oov_frag = {\"type\": frag_hash_to_type[frag_hash]}\n",
    "            oov_frag_hash = hash_frag(oov_frag)\n",
    "            if oov_frag_hash in vocab:\n",
    "                seq.append(token_to_id[oov_frag_hash])\n",
    "            else:\n",
    "                print(\"UNK_TOKEN\")\n",
    "                seq.append(token_to_id[UNK_TOKEN])\n",
    "\n",
    "    data.append([token_to_id[CLS_TOKEN]] + seq + [token_to_id[SEP_TOKEN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(data, open(\"ASTBERTa/data_subtrees.pkl\", \"wb\"))\n",
    "# pickle.dump(token_to_id, open(\"ASTBERTa/token_to_id.pkl\", \"wb\"))\n",
    "# pickle.dump(vocab, open(\"ASTBERTa/vocab.pkl\", \"wb\"))\n",
    "# pickle.dump(hash_to_frag, open(\"ASTBERTa/hash_to_frag.pkl\", \"wb\"))\n",
    "# pickle.dump(id_to_token, open(\"ASTBERTa/id_to_token.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "js-rl-vfj9GiAe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
