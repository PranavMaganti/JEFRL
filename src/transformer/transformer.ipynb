{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM\n",
    "\n",
    "from rl.tokenizer import ASTTokenizer\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "sys.setrecursionlimit(20000)\n",
    "setup_logging()\n",
    "MAX_FRAGMENT_SEQ_LEN = 512  # Maximum length of the AST fragment sequence\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "CLS_TOKEN = \"<s>\"\n",
    "SEP_TOKEN = \"</s>\"\n",
    "MASK_TOKEN = \"<mask>\"\n",
    "UNK_TOKEN = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible set difference at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 1\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "with open(\"../ASTBERTa/frag_data.pkl\", \"rb\") as f:\n",
    "    frag_data = pickle.load(f)\n",
    "\n",
    "with open(\"../ASTBERTa/vocab_data.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "\n",
    "frag_seqs = frag_data[\"frag_seqs\"]\n",
    "frag_id_to_type = frag_data[\"frag_id_to_type\"]\n",
    "frag_id_to_frag = frag_data[\"frag_id_to_frag\"]\n",
    "\n",
    "vocab = vocab_data[\"vocab\"]\n",
    "token_to_id = vocab_data[\"token_to_id\"]\n",
    "id_to_token = vocab_data[\"id_to_token\"]\n",
    "special_token_ids = vocab_data[\"special_token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = len(vocab)  # size of vocabulary\n",
    "intermediate_size = 2048  # embedding dimension\n",
    "hidden_size = 512\n",
    "\n",
    "num_hidden_layers = 3\n",
    "num_attention_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    intermediate_size=intermediate_size,\n",
    "    hidden_dropout_prob=dropout,\n",
    "    max_position_embeddings=MAX_FRAGMENT_SEQ_LEN + 2,\n",
    ")\n",
    "\n",
    "\n",
    "# # Load the ASTBERTa model\n",
    "tokenizer = ASTTokenizer(vocab, token_to_id, MAX_FRAGMENT_SEQ_LEN, device)\n",
    "pretrained_model = torch.load(\"../ASTBERTa/models/small_vocab/model_17500.pt\")\n",
    "# pretrained_model = torch.load(\"../ASTBERTa/models/final/model_27500.pt\")\n",
    "\n",
    "\n",
    "if isinstance(pretrained_model, torch.nn.DataParallel):\n",
    "    pretrained_model = pretrained_model.module\n",
    "\n",
    "# ast_net = RobertaForMaskedLM.from_pretrained(\n",
    "#     \"../ASTBERTa/models/new/checkpoint-35000\"\n",
    "# ).to(device)\n",
    "\n",
    "ast_net = RobertaForMaskedLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=None,\n",
    "    state_dict=pretrained_model.state_dict(),\n",
    "    config=config,\n",
    ").to(device)\n",
    "\n",
    "ast_net = BetterTransformer.transform(ast_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "\n",
    "def tokenize(frag_seq):\n",
    "    frag_id_seq: list[int] = []\n",
    "    frag_id_seq.append(token_to_id[CLS_TOKEN])\n",
    "\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        if frag_hash in token_to_id:\n",
    "            frag_id_seq.append(token_to_id[frag_hash])\n",
    "        else:\n",
    "            oov_frag: dict[str, str] = {\"type\": frag[\"type\"]}\n",
    "            oov_frag_hash = hash_frag(oov_frag)\n",
    "            if oov_frag_hash in token_to_id:\n",
    "                frag_id_seq.append(token_to_id[oov_frag_hash])\n",
    "            else:\n",
    "                print(f\"UNK_TOKEN: {frag_hash}\")\n",
    "                frag_id_seq.append(token_to_id[UNK_TOKEN])\n",
    "\n",
    "        if len(frag_id_seq) >= MAX_FRAGMENT_SEQ_LEN:\n",
    "            break\n",
    "\n",
    "    if len(frag_id_seq) < MAX_FRAGMENT_SEQ_LEN:\n",
    "        frag_id_seq.append(token_to_id[SEP_TOKEN])\n",
    "\n",
    "    random_start_idx = random.randint(1, len(frag_id_seq) - 1)\n",
    "    frag_id_seq = [token_to_id[CLS_TOKEN]] + frag_id_seq[\n",
    "        random_start_idx : random_start_idx + MAX_FRAGMENT_SEQ_LEN - 1\n",
    "    ]\n",
    "\n",
    "    return torch.tensor([frag_id_seq], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14017/14017 [01:04<00:00, 216.26it/s, acc=0.709]\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "\n",
    "for i, seq in (bar := tqdm.tqdm(enumerate(frag_seqs), total=len(frag_seqs))):\n",
    "    labels = tokenize(seq).to(device)\n",
    "    inputs = labels.clone()\n",
    "    attention_mask = torch.ones_like(inputs, device=device)\n",
    "\n",
    "    probability_matrix = torch.full(labels.shape, 0.15)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100\n",
    "    inputs[masked_indices] = token_to_id[MASK_TOKEN]\n",
    "\n",
    "    out = ast_net(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    preds = out.logits.argmax(dim=-1)\n",
    "    acc = (labels[masked_indices] == preds[masked_indices]).sum() / masked_indices.sum()\n",
    "\n",
    "    if not torch.isnan(acc):\n",
    "        accs.append(acc.item())\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        bar.set_postfix({\"acc\": sum(accs) / len(accs)})\n",
    "        acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7090650768758162\n"
     ]
    }
   ],
   "source": [
    "print(sum(accs) / len(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "js-rl-vfj9GiAe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
