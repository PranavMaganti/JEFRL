{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM\n",
    "\n",
    "from rl.tokenizer import ASTTokenizer\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "sys.setrecursionlimit(20000)\n",
    "setup_logging()\n",
    "MAX_FRAGMENT_SEQ_LEN = 512  # Maximum length of the AST fragment sequence\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "CLS_TOKEN = \"<s>\"\n",
    "SEP_TOKEN = \"</s>\"\n",
    "MASK_TOKEN = \"<mask>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible set difference at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 2\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/pranav/.cache/pypoetry/virtualenvs/js-rl-vfj9GiAe-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3505: FutureWarning: Possible nested set at position 1\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "with open(\"../ASTBERTa/frag_data.pkl\", \"rb\") as f:\n",
    "    frag_data = pickle.load(f)\n",
    "\n",
    "with open(\"../ASTBERTa/vocab_data.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "\n",
    "frag_seqs = frag_data[\"frag_seqs\"]\n",
    "frag_id_to_type = frag_data[\"frag_id_to_type\"]\n",
    "frag_id_to_frag = frag_data[\"frag_id_to_frag\"]\n",
    "\n",
    "vocab = vocab_data[\"vocab\"]\n",
    "token_to_id = vocab_data[\"token_to_id\"]\n",
    "id_to_token = vocab_data[\"id_to_token\"]\n",
    "special_token_ids = vocab_data[\"special_token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "\n",
    "def tokenize(frag_seq):\n",
    "    frag_id_seq: list[int] = []\n",
    "    frag_id_seq.append(token_to_id[CLS_TOKEN])\n",
    "\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        if frag_hash in token_to_id:\n",
    "            frag_id_seq.append(token_to_id[frag_hash])\n",
    "        else:\n",
    "            oov_frag: dict[str, str] = {\"type\": frag[\"type\"]}\n",
    "            oov_frag_hash = hash_frag(oov_frag)\n",
    "            if oov_frag_hash in token_to_id:\n",
    "                frag_id_seq.append(token_to_id[oov_frag_hash])\n",
    "            else:\n",
    "                print(f\"UNK_TOKEN: {frag_hash}\")\n",
    "                frag_id_seq.append(token_to_id[UNK_TOKEN])\n",
    "\n",
    "        if len(frag_id_seq) >= MAX_FRAGMENT_SEQ_LEN:\n",
    "            break\n",
    "\n",
    "    if len(frag_id_seq) < MAX_FRAGMENT_SEQ_LEN:\n",
    "        frag_id_seq.append(token_to_id[SEP_TOKEN])\n",
    "\n",
    "    random_start_idx = random.randint(1, len(frag_id_seq) - 1)\n",
    "    frag_id_seq = [token_to_id[CLS_TOKEN]] + frag_id_seq[\n",
    "        random_start_idx : random_start_idx + MAX_FRAGMENT_SEQ_LEN - 1\n",
    "    ]\n",
    "\n",
    "    return torch.tensor([frag_id_seq], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2283.53it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i, seq in (\n",
    "    bar := tqdm.tqdm(enumerate(frag_seqs[:1000]), total=len(frag_seqs[:1000]))\n",
    "):\n",
    "    labels = tokenize(seq).to(device)\n",
    "    inputs = labels.clone()\n",
    "    attention_mask = torch.ones_like(inputs, device=device)\n",
    "\n",
    "    probability_matrix = torch.full(labels.shape, 0.15)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100\n",
    "    inputs[masked_indices] = token_to_id[MASK_TOKEN]\n",
    "\n",
    "    data.append((inputs, attention_mask, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)  # size of vocabulary\n",
    "intermediate_size = 2048  # embedding dimension\n",
    "hidden_size = 512\n",
    "\n",
    "num_hidden_layers = 3\n",
    "num_attention_heads = 8\n",
    "dropout = 0\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    intermediate_size=intermediate_size,\n",
    "    hidden_dropout_prob=dropout,\n",
    "    max_position_embeddings=MAX_FRAGMENT_SEQ_LEN + 2,\n",
    ")\n",
    "\n",
    "\n",
    "# # Load the ASTBERTa model\n",
    "tokenizer = ASTTokenizer(vocab, token_to_id, MAX_FRAGMENT_SEQ_LEN, device)\n",
    "pretrained_model = torch.load(\n",
    "    \"../ASTBERTa/models/2023-06-20T14:44:.514456/model_8500.pt\"\n",
    ")\n",
    "# pretrained_model = torch.load(\"../ASTBERTa/models/final/model_27500.pt\")\n",
    "\n",
    "\n",
    "if isinstance(pretrained_model, torch.nn.DataParallel):\n",
    "    pretrained_model = pretrained_model.module\n",
    "\n",
    "# ast_net = RobertaForMaskedLM.from_pretrained(\n",
    "#     \"../ASTBERTa/models/new/chimport sklearn\n",
    "\n",
    "\n",
    "ast_net = RobertaForMaskedLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=None,\n",
    "    state_dict=pretrained_model.state_dict(),\n",
    "    config=config,\n",
    ").to(device)\n",
    "\n",
    "ast_net = BetterTransformer.transform(ast_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 64/1000 [00:00<00:02, 313.46it/s, loss=3.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 136/1000 [00:00<00:02, 342.22it/s, loss=3.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 212/1000 [00:00<00:02, 360.35it/s, loss=3.48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 295/1000 [00:00<00:01, 389.68it/s, loss=3.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 373/1000 [00:01<00:01, 379.74it/s, loss=3.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 450/1000 [00:01<00:01, 363.77it/s, loss=3.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 524/1000 [00:01<00:01, 362.53it/s, loss=3.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 643/1000 [00:01<00:00, 378.43it/s, loss=3.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 681/1000 [00:01<00:00, 378.17it/s, loss=3.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 797/1000 [00:02<00:00, 377.22it/s, loss=3.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 876/1000 [00:02<00:00, 381.96it/s, loss=3.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 953/1000 [00:02<00:00, 365.37it/s, loss=3.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n",
      "NaN loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 366.64it/s, loss=3.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN loss\n",
      "NaN loss\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "import tqdm\n",
    "from torchmetrics.classification import MulticlassF1Score, Accuracy\n",
    "\n",
    "losses = []\n",
    "\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "\n",
    "for inputs, attention_mask, labels in (bar := tqdm.tqdm(data, total=len(data))):\n",
    "    out = ast_net(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    loss = out.loss\n",
    "    if torch.isnan(loss):\n",
    "        print(\"NaN loss\")\n",
    "        continue\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    bar.set_postfix({\"loss\": sum(losses) / len(losses)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.406374173677297"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(losses) / len(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "js-rl-vfj9GiAe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
