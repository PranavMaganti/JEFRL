{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from preprocessing.normalise import collect_id, normalize_id\n",
    "from utils.loader import load_raw_corpus\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "sys.setrecursionlimit(20000)\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERM_TYPE = set(\n",
    "    [\n",
    "        \"DebuggerStatement\",\n",
    "        \"ThisExpression\",\n",
    "        \"Super\",\n",
    "        \"EmptyStatement\",\n",
    "        \"Import\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_raw_corpus(Path(\"../corpus/DIE\"))\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    id_idx = {\"v\": 0, \"f\": 0, \"c\": 0}\n",
    "    id_map = {}\n",
    "    collect_id(ast, id_map, id_idx)\n",
    "    normalize_id(ast, id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ASTBERTa/corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ASTBERTa/corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from js_ast.fragmentise import node_to_frags\n",
    "\n",
    "\n",
    "frag_seqs: list[list[dict[str, Any]]] = []\n",
    "frag_info_seqs: list[list[tuple[int, str]]] = []\n",
    "all_node_types: set[str] = set()\n",
    "\n",
    "for ast in tqdm.tqdm(corpus):\n",
    "    frag_seq: list[dict[str, Any]] = []\n",
    "    node_types: set[str] = set()\n",
    "\n",
    "    node_to_frags(ast, frag_seq, node_types)\n",
    "\n",
    "    frag_seqs.append(frag_seq)\n",
    "    all_node_types.update(node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_seqs_len = list(sorted(map(lambda x: len(x), frag_seqs)))\n",
    "frag_seqs_below_max = [frag_seqs for x in frag_seqs if len(x) < 512]\n",
    "\n",
    "print(\"Length of frag_seqs:\", len(frag_seqs))\n",
    "print(\"Node types:\", all_node_types)\n",
    "\n",
    "print(\"Max length of frag_seqs:\", frag_seqs_len[-100:])\n",
    "print(\"Min length of frag_seqs:\", frag_seqs_len[:5])\n",
    "print(\"Avg length of frag_seqs:\", sum(frag_seqs_len) / len(frag_seqs_len))\n",
    "print(\"Percentage of frag_seqs below 1024:\", len(frag_seqs_below_max) / len(frag_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "frag_freq: dict[str, int] = defaultdict(int)\n",
    "hash_to_frag: dict[str, dict[str, Any]] = {}\n",
    "frag_hash_to_type: dict[str, str] = {}\n",
    "\n",
    "for frag_seq in tqdm.tqdm(frag_seqs):\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        frag_freq[frag_hash] += 1\n",
    "\n",
    "        if frag_hash not in hash_to_frag:\n",
    "            hash_to_frag[frag_hash] = frag\n",
    "\n",
    "        if frag_hash not in frag_hash_to_type:\n",
    "            frag_hash_to_type[frag_hash] = frag[\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_freq_list = list(sorted(frag_freq.items(), reverse=True, key=lambda x: x[1]))\n",
    "oov_frags: list[str] = []\n",
    "\n",
    "# Add OOV anonymous frag type for those not in vocabulary\n",
    "for frag_type in all_node_types:\n",
    "    oov_frag = {\"type\": frag_type}\n",
    "    oov_frag_hash = hash_frag(oov_frag)\n",
    "    oov_frags.append(oov_frag_hash)\n",
    "    frag_hash_to_type[oov_frag_hash] = frag_type\n",
    "    hash_to_frag[oov_frag_hash] = oov_frag\n",
    "\n",
    "vocab_size = 20000\n",
    "\n",
    "threshold_frags = [frag_hash for frag_hash, freq in frag_freq_list if freq > 4]\n",
    "unique_vocab_frags = set(threshold_frags + oov_frags)\n",
    "\n",
    "vocab_frags = list(unique_vocab_frags)\n",
    "\n",
    "print(\"Number of unique fragments:\", len(frag_freq))\n",
    "print(\"Max frequency:\", max(frag_freq.values()))\n",
    "print(\"Min frequency:\", min(frag_freq.values()))\n",
    "\n",
    "print(\"Number of unique fragments with freq > 4:\", len(threshold_frags))\n",
    "print([hash_to_frag[frag] for frag in vocab_frags[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "CLS_TOKEN = \"<s>\"\n",
    "SEP_TOKEN = \"</s>\"\n",
    "MASK_TOKEN = \"<mask>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "special_tokens = [PAD_TOKEN, CLS_TOKEN, MASK_TOKEN, SEP_TOKEN, UNK_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ordered_vocab = special_tokens + list(vocab_frags)\n",
    "vocab = set(ordered_vocab)\n",
    "\n",
    "token_to_id = {token: i for i, token in enumerate(ordered_vocab)}\n",
    "id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "special_token_ids = set([token_to_id[token] for token in special_tokens])\n",
    "\n",
    "# Dictionary for fragment to type, special tokens are mapped to their string representation\n",
    "frag_id_to_type = {token_to_id[frag]: frag_hash_to_type[frag] for frag in vocab_frags}\n",
    "frag_id_to_frag = {token_to_id[frag]: hash_to_frag[frag] for frag in vocab_frags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_data = {\n",
    "    \"frag_seqs\": frag_seqs,\n",
    "    \"frag_id_to_type\": frag_id_to_type,\n",
    "    \"frag_id_to_frag\": frag_id_to_frag,\n",
    "}\n",
    "\n",
    "vocab_data = {\n",
    "    \"vocab\": vocab,\n",
    "    \"token_to_id\": token_to_id,\n",
    "    \"id_to_token\": id_to_token,\n",
    "    \"special_token_ids\": special_token_ids,\n",
    "}\n",
    "\n",
    "pickle.dump(frag_data, open(\"../ASTBERTa/frag_data_old.pkl\", \"wb\"))\n",
    "pickle.dump(vocab_data, open(\"../ASTBERTa/vocab_data_old.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ASTBERTa/frag_data_old.pkl\", \"rb\") as f:\n",
    "    frag_data = pickle.load(f)\n",
    "\n",
    "frag_seqs = frag_data[\"frag_seqs\"]\n",
    "frag_id_to_type = frag_data[\"frag_id_to_type\"]\n",
    "frag_id_to_frag = frag_data[\"frag_id_to_frag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frag_id_to_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(list(special_token_ids) + list(frag_id_to_type.keys()))\n",
    "print(len(vocab))\n",
    "\n",
    "token_to_id = {}\n",
    "\n",
    "for frag_id, frag in frag_id_to_frag.items():\n",
    "    token_to_id[hash_frag(frag)] = frag_id\n",
    "\n",
    "for i, item in enumerate(special_tokens):\n",
    "    token_to_id[item] = i\n",
    "\n",
    "id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "special_token_ids = set([token_to_id[token] for token in special_tokens])\n",
    "print(list(token_to_id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "with open(\"../ASTBERTa/frag_data.pkl\", \"rb\") as f:\n",
    "    frag_data = pickle.load(f)\n",
    "\n",
    "with open(\"../ASTBERTa/vocab_data.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "\n",
    "frag_seqs = frag_data[\"frag_seqs\"]\n",
    "frag_id_to_type = frag_data[\"frag_id_to_type\"]\n",
    "frag_id_to_frag = frag_data[\"frag_id_to_frag\"]\n",
    "\n",
    "vocab = vocab_data[\"vocab\"]\n",
    "token_to_id = vocab_data[\"token_to_id\"]\n",
    "id_to_token = vocab_data[\"id_to_token\"]\n",
    "special_token_ids = vocab_data[\"special_token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from js_ast.fragmentise import hash_frag\n",
    "\n",
    "data: list[list[int]] = []\n",
    "\n",
    "for frag_seq in tqdm.tqdm(frag_seqs):\n",
    "    seq: list[int] = []\n",
    "\n",
    "    for frag in frag_seq:\n",
    "        frag_hash = hash_frag(frag)\n",
    "        if frag_hash in vocab:\n",
    "            seq.append(token_to_id[frag_hash])\n",
    "        else:\n",
    "            oov_frag = {\"type\": frag_hash_to_type[frag_hash]}\n",
    "            oov_frag_hash = hash_frag(oov_frag)\n",
    "            if oov_frag_hash in vocab:\n",
    "                seq.append(token_to_id[oov_frag_hash])\n",
    "            else:\n",
    "                print(\"UNK_TOKEN\")\n",
    "                seq.append(token_to_id[UNK_TOKEN])\n",
    "\n",
    "    data.append([token_to_id[CLS_TOKEN]] + seq + [token_to_id[SEP_TOKEN]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(data, open(\"../ASTBERTa/data_old.pkl\", \"wb\"))\n",
    "# pickle.dump(token_to_id, open(\"ASTBERTa/token_to_id.pkl\", \"wb\"))\n",
    "# pickle.dump(vocab, open(\"ASTBERTa/vocab.pkl\", \"wb\"))\n",
    "# pickle.dump(hash_to_frag, open(\"ASTBERTa/hash_to_frag.pkl\", \"wb\"))\n",
    "# pickle.dump(id_to_token, open(\"ASTBERTa/id_to_token.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "js-rl-vfj9GiAe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
